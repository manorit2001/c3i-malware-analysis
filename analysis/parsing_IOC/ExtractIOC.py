import logging
import xml.etree.ElementTree as ET
from collections import defaultdict
from pathlib import Path

import pandas as pd

# needs wireshark-cli (tshark) installed
import pyshark
from tqdm import tqdm

IPV4 = 0x0800
IPV6 = 0x86DD


class ExtractIOC:
    def parse_network_info(self, pcap_file, file):
        all_frames = []
        logging.info("Parsing network info")
        with pyshark.FileCapture(pcap_file) as packets:
            for idx, packet in tqdm(enumerate(packets)):
                try:
                    cur_frame = {}

                    # all layers
                    cur_frame["protocols"] = " ".join(
                        [i.layer_name for i in packet.layers]
                    )
                    # frame layer
                    assert "frame" in packet
                    cur_frame["arrival_time"] = packet.frame.time_epoch  # arrival time

                    # eth layer
                    assert "eth" in packet
                    cur_frame["dest_mac"] = packet.eth.get("dst")
                    cur_frame["src_mac"] = packet.eth.get("src")

                    # ip layer
                    cur_frame["ip_dest"] = None
                    cur_frame["ip_src"] = None
                    cur_frame["ip_ver"] = None
                    cur_frame["ip_header_len"] = None

                    if int(packet.eth.get("type"), 16) == IPV4:
                        assert "ip" in packet

                        cur_frame["ip_dest"] = packet.ip.get("dst")
                        cur_frame["ip_src"] = packet.ip.get("src")
                        cur_frame["ip_ver"] = packet.ip.get("version")
                        cur_frame["ip_header_len"] = packet.ip.get("hdr_len")

                    elif int(packet.eth.get("type"), 16) == IPV6:
                        assert "ipv6" in packet

                        cur_frame["ip_dest"] = packet.ipv6.get("dst")
                        cur_frame["ip_src"] = packet.ipv6.get("src")
                        cur_frame["ip_ver"] = packet.ipv6.get("version")

                    # transport layer
                    cur_frame["transport_layer"] = packet.transport_layer
                    cur_frame["dest_port"] = packet[packet.transport_layer].get(
                        "dstport"
                    )
                    cur_frame["src_port"] = packet[packet.transport_layer].get(
                        "srcport"
                    )
                    # defaults
                    cur_frame["segment_length"] = None
                    cur_frame["tcp_flags"] = None

                    # customised transport layer
                    if "tcp" in packet:
                        cur_frame["segment_length"] = packet.tcp.get("len")
                        cur_frame["tcp_flags"] = packet.tcp.get(
                            "flags"
                        ).showname_value.replace(",", ".")
                    elif "udp" in packet:
                        cur_frame["segment_length"] = packet.udp.get("length")
                    all_frames.append(cur_frame)
                except AssertionError as e:
                    logging.warning(f"{idx}: could not parse packet")
                    logging.debug(packet.layers)
        df = pd.DataFrame(all_frames)
        df.to_csv(file)

    def SaveKeys(self, snapshot):
        registry_keys = {}
        for keys in snapshot:
            keys = keys.split(b"\r\n")
            key_name = keys[0]
            registry_keys[key_name] = []
            keys.pop(0)
            for key in keys:
                if key != b"":
                    registry_keys[key_name].append(key)
        return registry_keys

    def parse_registry_info(
        self, log_dir, file
    ):  # If interested to know the values that has been changed in the given key Type of regshot
        logging.info(f"Parse registry info")
        changes = open(file, "wb")
        for i in range(5):
            logging.info(f"Comparing diff in regshot_{i}")
            before = open(log_dir / ("regshot_before" + str(i + 1)), "rb")
            after = open(log_dir / ("regshot_after" + str(i + 1)), "rb")

            snapshot_1 = before.read()[2:].split(b"\r\n\r\n")
            snapshot_2 = after.read()[2:].split(b"\r\n\r\n")

            registry_keys_1 = self.SaveKeys(snapshot_1)
            registry_keys_2 = self.SaveKeys(snapshot_2)

            extra_keys = [
                key
                for key in registry_keys_2.keys()
                if key not in registry_keys_1.keys()
            ]
            deleted_keys = [
                key
                for key in registry_keys_1.keys()
                if key not in registry_keys_2.keys()
            ]
            common_keys = [
                key for key in registry_keys_1.keys() if key in registry_keys_2.keys()
            ]

            changes.write(b"Keys Added\n\n")
            for key in extra_keys:
                changes.write(key + b"\n")
                changes.write(b"".join(value + b"\n" for value in registry_keys_2[key]))

            changes.write(b"Keys Deleted\n\n")
            for key in deleted_keys:
                changes.write(key + b"\n")
                changes.write(b"".join(value + b"\n" for value in registry_keys_1[key]))

            changes.write(b"Keys Changed\n\n")
            for key in common_keys:
                if registry_keys_1[key] != registry_keys_2[key]:
                    changes.write(key + b"\n")
                    for value_1, value_2 in zip(
                        registry_keys_1[key], registry_keys_2[key]
                    ):
                        if value_1 != value_2:
                            changes.write(b"Before -> " + value_1 + b"\n")
                            changes.write(b"After -> " + value_2 + b"\n")
                    if len(registry_keys_1[key]) < len(registry_keys_2[key]):
                        changes.write(
                            b"\nAdded Values\n"
                            + b"".join(
                                value + b"\n"
                                for value in registry_keys_2[key]
                                if value not in registry_keys_1[key]
                            )
                        )
                        changes.write(b"\n")
                    if len(registry_keys_1[key]) > len(registry_keys_2[key]):
                        changes.write(
                            b"\nDeleted Values\n"
                            + b"".join(
                                value + b"\n"
                                for value in registry_keys_1[key]
                                if value not in registry_keys_2[key]
                            )
                        )
                        changes.write(b"\n")
            before.close()
            after.close()
        changes.close()

    def parse_api_hooks_info(self, xml_file, parsed_dir):
        logging.info("Parsing api hooks per process")
        process_dir = parsed_dir / "process"
        process_dir.mkdir(parents=False, exist_ok=True)

        root = ET.parse(xml_file).getroot()
        pids = defaultdict(list)
        for event in root.findall("eventlist/event"):
            pids[(event[2].text, event[3].text)].append([event[4].text, event[5].text])

        for pid in pids.keys():
            pid_file = open(process_dir / str(pid[0]), "w")
            pid_file.write(pid[0] + ":" + pid[1] + "\n\n\n")
            for op in pids[pid]:
                pid_file.write("event\n")
                pid_file.write(op[0] + "\n")
                if op[1] != None:
                    pid_file.write(op[1] + "\n\n")
                else:
                    pid_file.write("None\n\n")

            pid_file.close()
        logging.info(f"Files generated at {process_dir}")

    def convert_procmon_to_csv(self, file, output_csv):
        logging.info("Converting procmon.out to csv")
        with open(file) as f:
            content = f.read().strip()

        all_process = [
            [j.split(" : ")[-1].replace('"', "").strip() for j in i.strip().split("\n")]
            for i in content.split("\n\n")
        ]
        df = pd.DataFrame(
            all_process, columns=["Event", "Caption", "CommandLine", "Handle"]
        )
        df.to_csv(output_csv)
        logging.info(f"File generated at {output_csv}")

    def convert_apimon_to_csv(self, xml_file, parsed_dir):
        root = ET.parse(xml_file).getroot()
        all_events = []
        logging.info("Parsing eventlists")
        for event in tqdm(root.findall("eventlist/event")):
            event_details = {}
            for element in event.iter():
                if element.tag != event.tag:
                    event_details[element.tag] = element.text
            all_events.append(event_details)

        df = pd.DataFrame(all_events)
        df.to_csv(parsed_dir / "events.csv")
        del all_events

        all_process = []
        logging.info("Parsing processlist")
        for process in tqdm(root.findall("processlist/process")):
            process_details = {}
            process.remove(process.find("modulelist"))
            for element in process.iter():
                if element.tag != process.tag:
                    process_details[element.tag] = element.text
            all_process.append(process_details)

        df = pd.DataFrame(all_process)
        df.to_csv(parsed_dir / "process.csv")

    def parse_tcp_table_info(self, log_dir, parsed_dir):
        logging.info("Parsing tcp table info")

        formats = ["tcp6", "udp6", "tcp", "udp6"]
        base = "network_{}_{}.csv"
        outputs = [
            ("both", "unchanged.csv"),
            ("left_only", "closed.csv"),
            ("right_only", "opened.csv"),
        ]

        base_output_dir = parsed_dir / "network_info"

        first = None
        second = None
        for f in formats:
            x = pd.read_csv(log_dir / base.format("before", f))
            y = pd.read_csv(log_dir / base.format("after", f))

            # add format type info
            x["type"] = f
            y["type"] = f

            if first is None:
                first = x
                continue
            if second is None:
                second = y
                continue

            first = first.append(x)
            second = second.append(y)

        merged = first.merge(second, how="outer", indicator=True)

        for _type, filename in outputs:
            iters = (
                merged.loc[merged["_merge"] == _type]
                .drop("_merge", axis=1)
                .reset_index(drop=True)
                .groupby("pid")
            )

            for pid, df in iters:
                output_dir = base_output_dir / str(pid)
                output_dir.mkdir(parents=True, exist_ok=True)

                # output directory name will have pid info, not needed here
                df.drop("pid", axis=1).to_csv(output_dir / filename)
        logging.info(f"Files generated at {base_output_dir}")

    def run(self, log_dir, output_dir):
        log_dir = Path(log_dir)

        parsed_dir = Path(output_dir) / Path("parsed_dumps")
        parsed_dir.mkdir(parents=False, exist_ok=True)

        self.parse_network_info(
            str(log_dir / "log.pcapng"), str(parsed_dir / "network_changes.csv")
        )
        self.convert_procmon_to_csv(
            str(log_dir / "procmon.out"), str(parsed_dir / "procmon.csv")
        )
        self.parse_tcp_table_info(log_dir, parsed_dir)
        self.parse_registry_info(log_dir, parsed_dir / "registry_changes.txt")
        self.parse_api_hooks_info(log_dir / "apimon.xml", parsed_dir)
        self.convert_apimon_to_csv(log_dir / "apimon.xml", parsed_dir)
